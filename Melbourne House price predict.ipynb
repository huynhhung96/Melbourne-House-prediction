{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Hypotheses set for this Linear Regression model \n",
    "    Hypothesis 1: Houses has more rooms have higher price ('Room')\n",
    "    Hypothesis 2: Houses type 'h' has higher price than houses type 't', and houses type 't' have higher price than houses type 'u' ('Type')\n",
    "    Hypothesis 3: Houses in the central with smaller distance has higher price ('Distance')\n",
    "    Hypothesis 4: Houses has more parking space has higher price ('Car')\n",
    "    Hypothesis 5: Houses has larger land size has higher price ('Landsize')\n",
    "    Hypothesis 6: Houses has larger building area has higher price ('Building Area')\n",
    "    Hypothesis 7: Houses has long year used has lower price ('YearUsed')\n",
    "    Hypothesis 8: Houses is used more than 60 years and above has higher price ('Heritage')\n",
    "    Hypothesis 9: Houses price is different depend on the region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "pd.set_option('float_format', '{:,.2f}'.format)\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Downloads/Melbourne_housing_FULL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable 'Price' has 7610 missing values, accounting for ~22% total value. If consider these missing values by variable 'Method' value, we can conclude that there are 2 cases for the reason:\n",
    "1. Houses autionned can't match the price in the auction (PI, VB) or bidder withdrawn prior to auction (W) \n",
    "2. Houses sold not via auction (SP, SN, PN, SS, SA) or via auction but no data found (S)\n",
    "The first case has 2192 observation, accounting for ~6,29% total value. The second case has 5418 observation, accounting for ~ 15,54% observation. For the first case, we will drop all the missing values because these are incomplete transaction. For missing values in the second case, we will replace them by mean price value of 'Regionname' of that house.\n",
    "Ex: A house with missing 'Price' value, 'Method' type 'SP', 'Regionname' 'Sounthern Metropolian' will have a price value of 1,395,928.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean price value by region name\n",
    "mean_by_region = data.groupby('Regionname').Price.mean().sort_values(ascending = False)\n",
    "mean_by_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop missing value price cells with method is either ['PI', 'VB', 'W']\n",
    "data_2 = data[~((data.Price.isnull() == True) & (data.Method.isin(['PI', 'VB', 'W'])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing value price cells with mean_by_region value for method is either in the second case\n",
    "for region in mean_by_region.index:\n",
    "    data_2.loc[(data_2.Price.isnull() == True) & (data_2.Regionname == region), 'Price'] = mean_by_region[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop missing value in column with small contribution\n",
    "data_2 = data_2.dropna(subset = ['Distance', 'Postcode', 'CouncilArea', 'Regionname', 'Propertycount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop column which play no role in expecting a house's price\n",
    "data_2 = data_2.drop(columns = ['Lattitude', 'Longtitude', 'Postcode', 'Propertycount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that there're still a lot missing values in other column, there are 2 solutions we can think of:\n",
    "1. Use dropna right away\n",
    "2. Keep replace missing values by mean values:\n",
    "    + See that the correlation between 'Romms' and 'Bedroom2' is really hight (0,96) -> drop one, keep on. So we will drop 'Bedroom2'\n",
    "    + Bathroom: same as above, drop 'Bathroom' also\n",
    "    + Car: replace missing value by 'Type'. House type 'h' will have more parking space than house type 't' or 'u'\n",
    "    + Landsize: same as above\n",
    "    + Buildingarea: same as above\n",
    "    + YearBuild: Base on mean year build by 'Regionname' and 'Type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = data_2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conver 'Date' from str to Datetime\n",
    "data_2.Date = pd.to_datetime(data_2.Date, format = '%d/%m/%Y')\n",
    "data_3.Date = pd.to_datetime(data_3.Date, format = '%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix between variables\n",
    "data_2.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Price' has pretty high correlated value with 'Room', 'Distance', 'Bedroom2', 'Bathroom', 'YearBuilt'\n",
    "- 'Rooms' has really high correlated value with 'Bedroom2', 'Car' and 'Bedroom', so multicollinearity may happens between these 3 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_car = data_2.groupby('Type').Car.mean()\n",
    "mean_car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_BA = data_2.groupby('Type').BuildingArea.mean()\n",
    "mean_BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_LS = data_2.groupby('Type').Landsize.mean()\n",
    "mean_LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_YB = data_2.groupby(['Regionname','Type']).YearBuilt.mean()\n",
    "mean_YB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 'Bedroom2' and 'Bathroom' to avoid multicollinearity\n",
    "data_2 = data_2.drop(columns = ['Bedroom2', 'Bathroom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing value 'Car'\n",
    "for house_type in mean_car.index:\n",
    "    data_2.loc[(data_2.Car.isnull() == True) & (data_2.Type == house_type), 'Car'] = mean_car[house_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing value 'BuildingArea'\n",
    "for house_type in mean_BA.index:\n",
    "    data_2.loc[(data_2.BuildingArea.isnull() == True) & (data_2.Type == house_type), 'BuildingArea'] = mean_BA[house_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing value 'Landsize'\n",
    "for house_type in mean_LS.index:\n",
    "    data_2.loc[(data_2.Landsize.isnull() == True) & (data_2.Type == house_type), 'Landsize'] = mean_LS[house_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing value 'YearBuilt'\n",
    "for region in mean_YB.index.levels[0]:\n",
    "    for house_type in mean_YB[region].index:\n",
    "        data_2.loc[(data_2.YearBuilt.isnull() == True) & (data_2.Type == house_type) & (data_2.Regionname == region), 'YearBuilt'] = mean_YB[region][house_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Still there are 3 observations of 'YearBuilt' with Null values, we decide to drop it since it just take a small amount\n",
    "data_2 = data_2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.index = range(data_2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean price is at 1,067,376.83 with stanard deviation at 596,099.44\n",
    "- Mean price is pretty close to average price at 50%: 900,00.00 => standard distribution\n",
    "- Max price is at 11,200,000.00\n",
    "- Most of the price oscillate from 85,000.00 (min) to 1,395,928.33 (75%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.groupby('Rooms').Price.describe().sort_values('mean', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Houses have more room has higher price\n",
    "- But there's an exception: houses have about 7 - 9 rooms has its price dropped\n",
    "- This is partly because these houses don't have enough representative pattern\n",
    "- Houses with 3 rooms accounting for the most transaction, up to 14,271\n",
    "- Houses with 7 rooms and upper has a really low transaction rate, end up at 30 or even lower\n",
    "=> Hypothesis 1: Houses has more rooms have higher price ('Room')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.groupby('Type').Price.describe().sort_values('mean', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Houses type 'h' (cottage, villa, house, semi, terrace) have mean price at $1,192,671.09, higher than houses type 't'(townhouse) and 'u' (unit, duplex) \n",
    "=> Hypothesis 2: Houses type 'h' has higher price than houses type 't', and houses type 't' have higher price than houses type 'u' ('Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.groupby('Method').Price.describe().sort_values('mean', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Auction method doesn't really have a significant impact on house's price, since these means really close to one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.groupby(data_2.Date.dt.year).Price.describe().sort_values('mean', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Houses price from 2016 to 2018 don't really have a significant change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since 'Distance' is quite a distributed values, we could group it by a arbitrary range, here is every 5 unit\n",
    "dis = [-0.1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "dis = pd.cut(data_2.Distance, dis)\n",
    "data_2.groupby(dis).Price.agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Houses with distance from 15 and lower own the highest price\n",
    "- The farther (from the central), the lower the price\n",
    "- Most of the transaction come from houses with distance is from 20 and lower\n",
    "=> Hypothesis 3: Houses in the central with smaller distance has higher price ('Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since 'Car' is quite a distributed values, we could group it by a arbitrary range, here is every 5 unit\n",
    "car = [-0.1, 1, 2, 4, 6, 8, 10, 30]\n",
    "car = pd.cut(data_2.Car, car)\n",
    "data_2.groupby(car).Price.agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general, houses with more parking space (car) tend to have higher price, but this assumption is converse when the parking space value become to high\n",
    "=> Hypothesis 4: Houses has more parking space has higher price ('Car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since 'Landsize' is quite a distributed values, we could group it by a arbitrary range, here is every 5 unit\n",
    "ls = [-0.1, 150, 250, 350, 450, 550, 650, 750, 450000]\n",
    "ls = pd.cut(data_2.Landsize, ls)\n",
    "data_2.groupby(ls).Price.agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general observation, houses with larger land size has a higher price\n",
    "=> Hypothesis 5: Houses has larger land size has higher price ('Landsize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since 'BuildingArea' is quite a distributed values, we could group it by a arbitrary range, here is every 5 unit\n",
    "ba = [-0.1, 50, 150, 200, 250, 3200]\n",
    "ba = pd.cut(data_2.BuildingArea, ba)\n",
    "data_2.groupby(ba).Price.agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general observation, houses with larger building area has a higher price\n",
    "=> Hypothesis 6: Houses has larger building area has higher price ('Building Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the quality of the 'YearBuilt' variable, we create a variable 'YearUsed' by subtract date sale 'Date' for date built 'YearBuilt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['YearUsed'] = data_2.Date.dt.year - data_2['YearBuilt']\n",
    "data_3['YearUsed'] = data_3.Date.dt.year - data_3['YearBuilt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that there are some 'YearUsed' with negative value ( <0 ), we assume that this is a typo in data entry or these house are sold before having built. We will replace these value by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.loc[data_2.YearUsed<0, 'YearUsed'] = 0\n",
    "data_3.loc[data_3.YearUsed<0, 'YearUsed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yu = [-0.1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 850]\n",
    "yu = pd.cut(data_2.BuildingArea, yu)\n",
    "data_2.groupby(yu).Price.agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we can see that a house with a longer time used has its price drop down.\n",
    "But house with over 60 years used has a higher price due to it heritage aspect. We will create a dummy variable 'Heritage' for this case. If YearUsed > 60, Heritage is equal to 1 and equal to 0 when YearUsed < 60\n",
    "-> Hypothesis 7: Houses has long year used has lower price ('YearUsed')\n",
    "-> Hypothesis 8: Houses is used more than 60 years and above has higher price ('Heritage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Heritage variable\n",
    "data_2['Heritage'] = np.where(data_2.YearUsed > 60, 1, 0)\n",
    "data_3['Heritage'] = np.where(data_3.YearUsed > 60, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.groupby('Regionname').Price.describe().sort_values('mean', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "House price has a different depends on region, reach its highest at $1.395.928 at Southern Metropolitan\n",
    "-> Hypothesis 9: Houses price is different depend on the region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_2 - Type\n",
    "one_hot = pd.get_dummies(data_2.Type)\n",
    "data_2 = data_2.join(one_hot)\n",
    "data_2 = data_2.drop(columns = ['Type', 'u'])\n",
    "\n",
    "#data_2 - Regionname\n",
    "one_hot = pd.get_dummies(data_2.Regionname)\n",
    "data_2 = data_2.join(one_hot)\n",
    "data_2 = data_2.drop(columns = ['Regionname', 'Western Victoria'])\n",
    "\n",
    "#data_3 - Type\n",
    "one_hot = pd.get_dummies(data_2.Type)\n",
    "data_3 = data_3.join(one_hot)\n",
    "data_3 = data_3.drop(columns = ['Type', 'u'])\n",
    "\n",
    "#data_3 - Regionname\n",
    "one_hot = pd.get_dummies(data_2.Type)\n",
    "data_3 = data_3.join(one_hot)\n",
    "data_3 = data_3.drop(columns = ['Regionname', 'Western Victoria'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns that has no use for analystical\n",
    "data2 = data_2.drop(columns = ['Suburb', 'Address', 'Method', 'SellerG', 'Date', 'YearBuilt', 'CouncilArea'])\n",
    "data3 = data_3.drop(columns = ['Suburb', 'Address', 'Method', 'SellerG', 'Date', 'YearBuilt', 'CouncilArea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Holdout & data valiadation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create x and y set use in regression\n",
    "x_model2 = data_2.drop(columns = ['Price'])\n",
    "y_model2 = (data_2.Price)/1000\n",
    "\n",
    "x_model3 = data_3.drop(columns = ['Price'])\n",
    "y_model3 = (data_3.Price)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression OLS\n",
    "lin_reg = LinearRegression()\n",
    "x_model2_train, x_model2_test, y_model2_train, y_model2_test = train_test_split(x_model2, y_model2, test_size = 0.2, random_state = 0)\n",
    "lin_reg.fit(x_model2_train, y_model2_train)\n",
    "\n",
    "#make predict on test sample\n",
    "y_model2_pred = lin_reg.predict(x_model2_test)\n",
    "residuals_model2 = y_model2_pred - y_model2_test\n",
    "\n",
    "#print out summary report using statsmodels module\n",
    "x_model2_train2 = sm.add_constant(x_model2_train)\n",
    "est = sm.OLS(y_model2_train, x_model2_train2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.score(x_model2_test, y_model2_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lin_reg score is a negative value, means that our model has something wrong\n",
    "it could be that the model doesn't fit well with test set, or there's outlier in test set cause this\n",
    "-> draw a scatter plot to find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_ytest_model2 = pd.DataFrame({'residual': residuals_model2, 'y_pred': y_model2_pred})\n",
    "residual_ytest_model2.plot(kind = 'scatter', x = 'y_pred', y = 'residual', figsize = (20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that there are an observation with a really big value of residual regression index = 21472, we'll try to remove it to see the fit ability of the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_ytest_model2 = residual_ytest_model2[~(residual_ytest_model2.residual == residuals_model2.max())]\n",
    "residual_ytest_model2.plot(kind = 'scatter', x = 'y_pred', y = 'residual', figsize = (20,10))\n",
    "lin_reg.score(x_model2_test.drop(21472, axis = 0), y_model2_test.drop(21472, axis = 0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removed the outlier, the model fit well with R2 = 0,44. We will remove it from the dataset before hand on other analysis\n",
    "From the scatter chart, we can see that the residual has a heterogeneous variance. House with low price has small residual relatively. House with higher price has big and distributed residual "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we run regression OLS on data_3 (the one we dropna completely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression OLS\n",
    "lin_reg2 = LinearRegression()\n",
    "x_model3_train, x_model3_test, y_model3_train, y_model3_test = train_test_split(x_model3, y_model3, test_size = 0.2, random_state = 0)\n",
    "lin_reg2.fit(x_model3_train, y_model3_train)\n",
    "\n",
    "#make predict on test sample\n",
    "y_model3_pred = lin_reg2.predict(x_model3_test)\n",
    "residual_model3 = y_model3_pred - y_model3_test\n",
    "\n",
    "#print out summary report using statsmodels module\n",
    "x_model3_train2 = sm.add_constant(x_model3_train)\n",
    "est = sm.OLS(y_model3_train, x_model3_train2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg2.score(x_model3_test, y_model3_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the outlier value with index = 21472\n",
    "x_model2 = x_model2.drop(index = 21472)\n",
    "x_model3 = x_model3.drop(index = 21472)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply model with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply OLS model with cross validation on data_2\n",
    "sum_R2 = 0\n",
    "count_R2 = 0\n",
    "for test in range(10):\n",
    "    lin_Reg = LinearRegression()\n",
    "    x_model2_train, x_model2_test, y_model2_train, y_model2_test = train_test_split(x_model2, y_model2, test_size = 0.2)\n",
    "    lin_reg.fit(x_model2_train, y_model2_train)\n",
    "    sum_R2 += lin_reg.score(x_model2_test, y_model2_test)\n",
    "    count R2 += 1\n",
    "    prince('# R2 score of test %d: %.2f' %(test, lin_Reg.score(x_model2_test, y_model2_test)))\n",
    "print('# Average R2 score: %.2f' %(sum_R2/ count_R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average fit ability of OLS model toward data_2 reach 0,50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply OLS model with cross validation on data_3\n",
    "sum_R2 = 0\n",
    "count_R2 = 0\n",
    "for test in range(10):\n",
    "    lin_reg = LinearRegression()\n",
    "    x_model3_train, x_model3_test, y_model3_train, y_model3_test = train_test_split(x_model3, y_model3, test_size = 0.2)\n",
    "    lin_reg.fit(x_model3_train, y_model3_train)\n",
    "    sum_R2 += lin_reg.score(x_model3_test, y_model3_test)\n",
    "    count_R2 += 1\n",
    "    print(' # R2 score of %d test: %.2f' %(test, lin_reg.score(x_model3_test, y_model3_test)))\n",
    "print('# Average R2 score: %.2f' %(sum_R2/count_R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average fit ability of OLS model toward data_3 reach 0,55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Regression model OLS has a stastical meaning when p-value of its variables is very small and p-value of F-test approximately to 0. However we can not tell much from the model because R2 score is about 0,5 - 0,55, means that the model can just explain almost a half the fluctuation of the house price. Residual scatter plot also showed that regression residual has homoscedasticity (hiện tượng phương sai k đồng nhất). Correlation between independant variables signal of multicollinearity\n",
    "This could be contributed by a removed variable 'SellerG' that hasn't been clearly explained and analyzied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV. PCA Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the dimension need to be reduced in PCA on data_2\n",
    "pca = PCA()\n",
    "x_model2_reduced = pca.fit_transform(scale(x_model2))\n",
    "pca_var_explain = pd.Series(np.round(pca.explained_variance_ratio_ *100, decimals=2))\n",
    "pca_var_explain.plot(kind = 'bar')\n",
    "pca_var_explain.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, we see that when k = 10, we could obtain 92% information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the dimension need to be reduced in PCA on data_3\n",
    "pca_data3 = PCA()\n",
    "x_model3_reduced = pca_data3.fit_transform(scale(x_model3))\n",
    "pca_var_explain_data3 = pd.Series(np.round(pca.explained_variance_ratio_ *100, decimals=2))\n",
    "pca_var_explain_data3.plot(kind = 'bar')\n",
    "pca_var_explain_data3.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, we see that when k = 10, we could obtain 90,64% information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Regression on data_2\n",
    "sum_R2 = 0\n",
    "count_R2 = 0\n",
    "x_model2_reduced = PCA(n_components = 10).fit_transform(scale(x_model2))\n",
    "for test in range(10):\n",
    "    lin_reg = LinearRegression()\n",
    "    x_model2_train, x_model2_test, y_model2_train, y_model2_test = train_test_split(x_model2_reduced, y_model2, test_size = 0.2)\n",
    "    lin_reg.fit(x_model2_train, y_model2_train)\n",
    "    sum_R2 += lin_reg.score(x_model2_test, y_model2_test)\n",
    "    count R2 += 1\n",
    "    print('# R2 score of %d test: %.2f' %(test, lin_reg.score(x_model2_test, y_model2_test)))\n",
    "print('# Average R2 score: %.2f' %(sum_R2/count_R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result doesn't bring much change compare to Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Regression on data_3\n",
    "sum_R2 = 0\n",
    "count_R2 = 0\n",
    "x_model3_reduced = PCA(n_components = 10).fit_transform(scale(x_model3))\n",
    "for test in range(10):\n",
    "    lin_reg = LinearRegression()\n",
    "    x_model3_train, x_model3_test, y_model3_train, y_model3_test = train_test_split(x_model3_reduced, y_model3, test_size = 0.2)\n",
    "    lin_reg.fit(x_model3_train, y_model3_train)\n",
    "    sum_R2 += lin_reg.score(x_model3_test, y_model3_test)\n",
    "    count R2 += 1\n",
    "    print('# R2 score of %d test: %.2f' %(test, lin_reg.score(x_model2_test, y_model2_test)))\n",
    "print('# Average R2 score: %.2f' %(sum_R2/count_R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: The regression result after applied PCA doesn't have much improvement because its dimension values doesn't have an outstanding value compare to the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V. Tuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tuning our model because we have 2 problems: a variable 'SellerG' is left out, and homoscedasticity \n",
    "We add in 'SellerG' using a ordinal variable, since the original has type 'category', called 'SellerG_idx' with value [1,2,3,4] when the average sell price of the seller fall into ranges: [min_,Q1), [Q1,Q2), [Q2,Q3), [Q3,max_)\n",
    "In addition, we take logarit of 'Price', 'Distance', 'Landsize' and 'BuildingArea' in logarit regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove variable that has no use in our model and drop the outlier value\n",
    "data_2 = data_2.drop(columns = ['Suburb', 'Method', 'Date', 'YearBuilt', 'CouncilArea'])\n",
    "data_2 = data_2.drop(index = 21472)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex data\n",
    "data_2 = range(data_2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.groupby('SellerG').Price.mean().describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#milestone for encoding SellerG\n",
    "min_ = data_2.groupby('SellerG').Price.mean().min()\n",
    "Q1_ = data_2.groupby('SellerG').Price.mean().quantile(0.25)\n",
    "Q2_ = data_2.groupby('SellerG').Price.mean().quantile(0.5)\n",
    "Q3_ = data_2.groupby('SellerG').Price.mean().quantile(0.75)\n",
    "max_ = data_2.groupby('SellerG').Price.mean().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode SellerG\n",
    "temp = pd.Series(data_2.groupby('SellerG').Price.mean())\n",
    "mean_price_by_seller = pd.DataFrame({'SellerG': temp.index, 'mean_price':temp.values, 'SellerG_idx':0})\n",
    "for i in range(len(temp.index)):\n",
    "    mean_price = mean_price_by_seller.loc[i, 'mean_price']\n",
    "    sellerg_idx = mean_price_by_seller.loc[i, 'SellerG_idx']\n",
    "    if mean_price < Q1\n",
    "        mean_price_by_seller.loc[i, 'SellerG_idx'] = 1\n",
    "    elif mean_price < Q2:\n",
    "        mean_price_by_seller.loc[i, 'SellerG_idx'] = 2\n",
    "    elif mean_price < Q3:\n",
    "        mean_price_by_seller.loc[i, 'SellerG_idx'] = 3\n",
    "    elif mean_price < Q4:\n",
    "        mean_price_by_seller.loc[i, 'SellerG_idx'] = 4\n",
    "        \n",
    "for record in mean_price_seller.values:\n",
    "    data_2.loc[data_2.SellerG == record[0], 'SellerG'] = record[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarit 'Price', 'Distance', 'Landsize', 'BuildingArea'\n",
    "#Remove jamming with 0 values\n",
    "data_2 = data_2[~(data_2.Distance == 0)]\n",
    "data_2 = data_2[~(data_2.Langsize == 0)]\n",
    "data_2 = data_2[~(data_2.BuildingArea == 0)]\n",
    "\n",
    "#Take logarit\n",
    "data_2.Price = np.log(data_2.Price)\n",
    "data_2.Distance = np.log(data_2.Distance)\n",
    "data_2.Landsize = np.log(data_2.Landsize)\n",
    "data_2.BuildingArea = np.log(data_2.BuildingArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model4 = data_2.drop(column=['Price', 'Address'])\n",
    "y_model4 = (data_2.Price)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS Regression on data_2\n",
    "lin_reg = LinearRegression()\n",
    "x_model4_train, x_model4_test, y_model4_train, y_model4_test = train_test_split(x_model4, y_model4, test_size = 0.2, randomstate = 0)\n",
    "lin_reg.fit(x_model4_train, y_model4_train)\n",
    "\n",
    "#make prediction on test sample\n",
    "y_model4_pred = lin-reg.predict(x_model4_test)\n",
    "residuals_model4 = y_model4_pred - y_model4_test\n",
    "\n",
    "#print summary using statsmodels module\n",
    "x_model4_train2 = sm.add_constant(x_model4_train)\n",
    "est = sm.OLS(y_model4_train, x_model4_train2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_ytest_model4 = pd.DataFrame({'residual:'residuals_model4, 'y_pred':y_model4_pred})\n",
    "residual_ytest_model4.plot(kind = 'scatter', x ='y_pred', y ='residual', figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply OLS model with cross validation on data_2\n",
    "sum_R2 = 0\n",
    "count_R2 = 0\n",
    "for test in range(10):\n",
    "    lin_reg = LinearRegression()\n",
    "    x_model4_train, x_model4_test, y_model4_train, y_model4_test = train_test_split(x_model4_reduced, y_model4, test_size = 0.2)\n",
    "    lin_reg.fit(x_model4_train, y_model4_train)\n",
    "    sum_R2 += lin_reg.score(x_model4_test, y_model4_test)\n",
    "    count R2 += 1\n",
    "    print('# R2 score of %d test: %.2f' %(test, lin_reg.score(x_model4_test, y_model4_test)))\n",
    "print('# Average R2 score: %.2f' %(sum_R2/count_R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can tell that add 'SellerG' variable in and take logarit numeric variables improve the explain ability of our model. Apply cross validation on test sample result an average score at 0.64, much improvement since before tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the dimension need to be reduced in PCA on data_2\n",
    "pca = PCA()\n",
    "x_model4_reduced = pca.fit_transform(scale(x_model4))\n",
    "pca_var_explain = pd.Series(np.round(pca.explained_variance_ratio_ *100, decimals=2))\n",
    "pca_var_explain.plot(kind = 'bar')\n",
    "pca_var_explain.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply PCA regression on data_2\n",
    "sum_R2 = 0\n",
    "count_R2 = 0\n",
    "x_model4_reduced = PCA(n_components = 10).fit_transform(scale(x_model4))\n",
    "for test in range(10):\n",
    "    lin_reg = LinearRegression()\n",
    "    x_model4_train, x_model4_test, y_model4_train, y_model4_test = train_test_split(x_model4_reduced, y_model4, test_size = 0.2)\n",
    "    lin_reg.fit(x_model3_train, y_model3_train)\n",
    "    sum_R2 += lin_reg.score(x_model4_test, y_model4_test)\n",
    "    count R2 += 1\n",
    "    print('# R2 score of %d test: %.2f' %(test, lin_reg.score(x_model4_test, y_model4_test)))\n",
    "print('# Average R2 score: %.2f' %(sum_R2/count_R2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
